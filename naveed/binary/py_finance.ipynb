{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Performance Loan Risk Prediction (GPU Accelerated)\n",
    "\n",
    "**Hardware:** Designed for NVIDIA GPUs (RTX 30xx/40xx series recommended).\n",
    "\n",
    "### Architectures:\n",
    "1.  **Deep Residual Neural Network (ResNet-MLP)**: Uses residual skip connections, Batch Normalization, and GELU activations. Optimized with `OneCycleLR` scheduler.\n",
    "2.  **High-Fidelity Approximation SVM**: Uses a high-dimensional Nystroem feature map (`n_components=2500`) to closely mimic a true RBF kernel, solved via Linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Processing Unit: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "   Device Tensor: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"âš¡ Processing Unit: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"   Device Tensor: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data Processed. Input Dimensions: 22 features.\n"
     ]
    }
   ],
   "source": [
    "train_path = 'finance/train_updated.csv'\n",
    "test_path = 'finance/test_updated.csv'\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Files not found. Ensure 'finance/train_updated.csv' exists.\")\n",
    "    raise\n",
    "\n",
    "# --- Preprocessing Setup ---\n",
    "target = 'RiskFlag'\n",
    "id_col = 'ProfileID'\n",
    "\n",
    "X = train_df.drop([target, id_col], axis=1)\n",
    "y = train_df[target]\n",
    "X_test_submission = test_df.drop([id_col], axis=1)\n",
    "test_ids = test_df[id_col]\n",
    "\n",
    "# 1. Binary Mapping\n",
    "binary_cols = ['OwnsProperty', 'FamilyObligation', 'JointApplicant']\n",
    "for col in binary_cols:\n",
    "    X[col] = X[col].map({'Yes': 1, 'No': 0})\n",
    "    X_test_submission[col] = X_test_submission[col].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# 2. Ordinal Encoding (Preserving Rank Information)\n",
    "qualification_order = [\"High School\", \"Bachelor's\", \"Master's\", \"PhD\"]\n",
    "workcategory_order = [\"Unemployed\", \"Part-time\", \"Full-time\", \"Self-employed\"]\n",
    "\n",
    "ordinal_cols = ['QualificationLevel', 'WorkCategory']\n",
    "ordinal_encoder = OrdinalEncoder(categories=[qualification_order, workcategory_order], \n",
    "                                 handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "X[ordinal_cols] = ordinal_encoder.fit_transform(X[ordinal_cols])\n",
    "X_test_submission[ordinal_cols] = ordinal_encoder.transform(X_test_submission[ordinal_cols])\n",
    "\n",
    "# 3. Full Transformation Pipeline\n",
    "nominal_cols = ['RelationshipStatus', 'FundUseCase']\n",
    "numerical_cols = [c for c in X.columns if c not in ordinal_cols + nominal_cols + binary_cols]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('nom', OneHotEncoder(handle_unknown='ignore', sparse_output=False), nominal_cols),\n",
    "        ('ord', 'passthrough', ordinal_cols),\n",
    "        ('bin', 'passthrough', binary_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "X_test_processed = preprocessor.transform(X_test_submission)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_processed, y, test_size=0.15, stratify=y, random_state=42)\n",
    "\n",
    "print(f\"âœ… Data Processed. Input Dimensions: {X_train.shape[1]} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. High-Fidelity SVM (Nystroem Approximation)\n",
    "\n",
    "We scale up `n_components` to 2500. This creates a very rich feature map that mimics the non-linear RBF kernel almost perfectly, allowing the LinearSVC to find complex decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Training High-Fidelity SVM... (This uses CPU but is optimized)\n",
      "âœ… SVM Training Complete. Validation ROC-AUC: 0.7464\n"
     ]
    }
   ],
   "source": [
    "print(\"â³ Training High-Fidelity SVM... (This uses CPU but is optimized)\")\n",
    "\n",
    "# 1. Feature Map: Projects data into 2500 dimensions to approximate RBF kernel\n",
    "feature_map = Nystroem(kernel='rbf', gamma=0.1, n_components=2500, random_state=42)\n",
    "\n",
    "# 2. Classifier: Linear SVM on the projected features\n",
    "svm_clf = LinearSVC(dual=False, C=1.0, class_weight='balanced', max_iter=2000, random_state=42)\n",
    "\n",
    "# 3. CalibratedClassifierCV: Wraps SVM to give us actual probabilities (predict_proba) instead of just labels\n",
    "svm_pipeline = Pipeline([\n",
    "    ('feature_map', feature_map),\n",
    "    ('svm_calibrated', CalibratedClassifierCV(svm_clf, cv=3)) \n",
    "])\n",
    "\n",
    "svm_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Validation\n",
    "y_val_probs_svm = svm_pipeline.predict_proba(X_val)[:, 1]\n",
    "auc_score = roc_auc_score(y_val, y_val_probs_svm)\n",
    "print(f\"âœ… SVM Training Complete. Validation ROC-AUC: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Residual Neural Network (ResNet-MLP)\n",
    "\n",
    "A standard MLP struggles when it gets too deep. We use **Residual Blocks** (skip connections) to allow gradients to flow easily, enabling us to use a deeper, more powerful architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Deep Learning Training...\n",
      "Epoch 1/40 | Train Loss: 0.3330 | Val ROC-AUC: 0.7517\n",
      "Epoch 2/40 | Train Loss: 0.3168 | Val ROC-AUC: 0.7539\n",
      "Epoch 3/40 | Train Loss: 0.3163 | Val ROC-AUC: 0.7527\n",
      "Epoch 4/40 | Train Loss: 0.3161 | Val ROC-AUC: 0.7551\n",
      "Epoch 5/40 | Train Loss: 0.3156 | Val ROC-AUC: 0.7533\n",
      "Epoch 6/40 | Train Loss: 0.3155 | Val ROC-AUC: 0.7541\n",
      "Epoch 7/40 | Train Loss: 0.3150 | Val ROC-AUC: 0.7546\n",
      "Epoch 8/40 | Train Loss: 0.3150 | Val ROC-AUC: 0.7539\n",
      "Epoch 9/40 | Train Loss: 0.3149 | Val ROC-AUC: 0.7529\n",
      "â¹ï¸ Early stopping triggered.\n",
      "ðŸ† Best Val AUC: 0.7551\n"
     ]
    }
   ],
   "source": [
    "# --- Dataset Class ---\n",
    "class LoanDataset(Dataset):\n",
    "    def __init__(self, features, labels=None):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.float32).unsqueeze(1) if labels is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.features[idx], self.labels[idx]\n",
    "        return self.features[idx]\n",
    "\n",
    "# --- Residual Block Architecture ---\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_features, dropout_rate=0.3):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features),\n",
    "            nn.BatchNorm1d(n_features),\n",
    "            nn.GELU(), # GELU is often better than ReLU for deep networks\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(n_features, n_features),\n",
    "            nn.BatchNorm1d(n_features)\n",
    "        )\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.block(x)\n",
    "        out += identity  # Skip Connection\n",
    "        return self.activation(out)\n",
    "\n",
    "class DeepRiskNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepRiskNet, self).__init__()\n",
    "        \n",
    "        # Initial projection\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Stack of Residual Blocks\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResBlock(512, dropout_rate=0.4),\n",
    "            ResBlock(512, dropout_rate=0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            ResBlock(256, dropout_rate=0.3),\n",
    "            ResBlock(256, dropout_rate=0.3)\n",
    "        )\n",
    "        \n",
    "        # Output Head\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.output_head(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# --- Hyperparameters for RTX 4060 ---\n",
    "BATCH_SIZE = 1024  # Large batch size for GPU efficiency\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "# Loaders\n",
    "train_dataset = LoanDataset(X_train, y_train)\n",
    "val_dataset = LoanDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Model Init\n",
    "model = DeepRiskNet(X_train.shape[1]).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "# OneCycle Scheduler (Super-convergence)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, steps_per_epoch=len(train_loader), epochs=EPOCHS)\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"ðŸš€ Starting Deep Learning Training...\")\n",
    "best_val_auc = 0\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_val_preds = []\n",
    "    all_val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_val_b, y_val_b in val_loader:\n",
    "            X_val_b = X_val_b.to(device)\n",
    "            preds = model(X_val_b)\n",
    "            all_val_preds.extend(preds.cpu().numpy())\n",
    "            all_val_labels.extend(y_val_b.numpy())\n",
    "    \n",
    "    val_auc = roc_auc_score(all_val_labels, all_val_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss/len(train_loader):.4f} | Val ROC-AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    # Early Stopping Check\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"â¹ï¸ Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best model for inference\n",
    "model.load_state_dict(torch.load('best_model.pth', weights_only=True))\n",
    "print(f\"ðŸ† Best Val AUC: {best_val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Separate Submission Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated 'submission_nn.csv' (Neural Network)\n",
      "âœ… Generated 'submission_svm.csv' (SVM)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Generate Neural Network Predictions ---\n",
    "test_tensor = torch.tensor(X_test_processed, dtype=torch.float32).to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    nn_probs = model(test_tensor).cpu().numpy().flatten()\n",
    "    nn_predictions = (nn_probs > 0.5).astype(int)\n",
    "\n",
    "submission_nn = pd.DataFrame({\n",
    "    'ProfileID': test_ids,\n",
    "    'RiskFlag': nn_predictions\n",
    "})\n",
    "submission_nn.to_csv('submission_nn.csv', index=False)\n",
    "print(\"âœ… Generated 'submission_nn.csv' (Neural Network)\")\n",
    "\n",
    "# --- 2. Generate SVM Predictions ---\n",
    "# Use the pipeline we trained earlier\n",
    "svm_predictions = svm_pipeline.predict(X_test_processed)\n",
    "\n",
    "submission_svm = pd.DataFrame({\n",
    "    'ProfileID': test_ids,\n",
    "    'RiskFlag': svm_predictions\n",
    "})\n",
    "submission_svm.to_csv('submission_svm.csv', index=False)\n",
    "print(\"âœ… Generated 'submission_svm.csv' (SVM)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
