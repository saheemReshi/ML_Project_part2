{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚ö° Processing Unit: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"   Device Tensor: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'finance/train_updated.csv'\n",
    "test_path = 'finance/test_updated.csv'\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Files not found. Ensure 'finance/train_updated.csv' exists.\")\n",
    "    raise\n",
    "\n",
    "\n",
    "target = 'RiskFlag'\n",
    "id_col = 'ProfileID'\n",
    "\n",
    "X = train_df.drop([target, id_col], axis=1)\n",
    "y = train_df[target]\n",
    "X_test_submission = test_df.drop([id_col], axis=1)\n",
    "test_ids = test_df[id_col]\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "n = len(num_cols)\n",
    "cols = 3                                           # number of columns you want\n",
    "rows = math.ceil(n / cols)                         # auto compute rows needed\n",
    "\n",
    "plt.figure(figsize=(6 * cols, 4 * rows))\n",
    "\n",
    "for idx, col in enumerate(num_cols, 1):\n",
    "    plt.subplot(rows, cols, idx)\n",
    "    sns.boxplot(y=X[col])\n",
    "    plt.title(col, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Figure 1: Boxplots for numeric features\", fontsize=14, y=0.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6 * cols, 4 * rows))\n",
    "\n",
    "for idx, col in enumerate(num_cols, 1):\n",
    "    plt.subplot(rows, cols, idx)\n",
    "    plt.hist(X[col], bins=30)\n",
    "    plt.title(col, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Figure 2: Histograms showing feature distributions\", fontsize=14, y=0.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "desc = X.describe().T  # transpose so each feature is a row\n",
    "cols_per_table = 2     # change to 3 if you prefer wider tables\n",
    "tables = []\n",
    "\n",
    "for i in range(0, len(desc), cols_per_table):\n",
    "    chunk = desc.iloc[i:i + cols_per_table]\n",
    "    tables.append(chunk)\n",
    "\n",
    "# Display chunks\n",
    "for idx, t in enumerate(tables, 1):\n",
    "    print(f\"\\n--- Table {idx} ---\\n\")\n",
    "    display(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = ['OwnsProperty', 'FamilyObligation', 'JointApplicant']\n",
    "for col in binary_cols:\n",
    "    X[col] = X[col].map({'Yes': 1, 'No': 0})\n",
    "    X_test_submission[col] = X_test_submission[col].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "\n",
    "qualification_order = [\"High School\", \"Bachelor's\", \"Master's\", \"PhD\"]\n",
    "workcategory_order = [\"Unemployed\", \"Part-time\", \"Full-time\", \"Self-employed\"]\n",
    "\n",
    "ordinal_cols = ['QualificationLevel', 'WorkCategory']\n",
    "ordinal_encoder = OrdinalEncoder(categories=[qualification_order, workcategory_order], \n",
    "                                 handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "X[ordinal_cols] = ordinal_encoder.fit_transform(X[ordinal_cols])\n",
    "X_test_submission[ordinal_cols] = ordinal_encoder.transform(X_test_submission[ordinal_cols])\n",
    "\n",
    "\n",
    "nominal_cols = ['RelationshipStatus', 'FundUseCase']\n",
    "numerical_cols = [c for c in X.columns if c not in ordinal_cols + nominal_cols + binary_cols]\n",
    "\n",
    "\n",
    "\n",
    "print(\" Identifying Outliers \")\n",
    "Q1 = X[numerical_cols].quantile(0.25)\n",
    "Q3 = X[numerical_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outlier_condition = ((X[numerical_cols] < (Q1 - 1.5 * IQR)) | (X[numerical_cols] > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "\n",
    "X = X[~outlier_condition]\n",
    "y = y[~outlier_condition]\n",
    "print(f\"‚úÖ Removed {outlier_condition.sum()} outliers. Remaining: {len(X)} samples.\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('nom', OneHotEncoder(handle_unknown='ignore', sparse_output=False), nominal_cols),\n",
    "        ('ord', 'passthrough', ordinal_cols),\n",
    "        ('bin', 'passthrough', binary_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "X_test_processed = preprocessor.transform(X_test_submission)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_processed, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "print(f\"‚úÖ Data Processed. Input Dimensions: {X_train.shape[1]} features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20% dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mini_processed, _, y_mini, _ = train_test_split(\n",
    "    X_processed, y,\n",
    "    test_size=0.8,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_mini_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "print(\"‚è≥ Generating EDA Profile Report... \")\n",
    "\n",
    "\n",
    "profile = ProfileReport(\n",
    "    train_df, \n",
    "    title=\"Loan Risk EDA Report\", \n",
    "    minimal=True, \n",
    "    explorative=True\n",
    ")\n",
    "\n",
    "profile.to_file(\"finance_eda_report.html\")\n",
    "\n",
    "print(\"‚úÖ Report generated: 'finance_eda_report.html'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚è≥ Training SVM... \")\n",
    "\n",
    "\n",
    "feature_map = Nystroem(kernel='rbf', gamma=0.1, n_components=2500, random_state=42)\n",
    "\n",
    "\n",
    "svm_clf = LinearSVC(dual=False, C=1.0, class_weight='balanced', max_iter=2000, random_state=42)\n",
    "\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    ('feature_map', feature_map),\n",
    "    ('svm_calibrated', CalibratedClassifierCV(svm_clf, cv=3)) \n",
    "])\n",
    "\n",
    "svm_pipeline.fit(X_processed, y)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"‚úÖ SVM Training Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression & Bayesian Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚è≥ Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_processed, y)\n",
    "\n",
    "# y_val_probs_lr = lr_model.predict_proba(X_val)[:, 1]\n",
    "# lr_auc = roc_auc_score(y_val, y_val_probs_lr)\n",
    "print(f\"‚úÖ Logistic Regression Complete\")\n",
    "\n",
    "\n",
    "print(\"‚è≥ Training Gaussian Naive Bayes...\")\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# y_val_probs_nb = nb_model.predict_proba(X_val)[:, 1]\n",
    "# nb_auc = roc_auc_score(y_val, y_val_probs_nb)\n",
    "print(f\"‚úÖ Naive Bayes Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LoanDataset(Dataset):\n",
    "    def __init__(self, features, labels=None):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.float32).unsqueeze(1) if labels is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.features[idx], self.labels[idx]\n",
    "        return self.features[idx]\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_features, dropout_rate=0.3):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features),\n",
    "            nn.BatchNorm1d(n_features),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(n_features, n_features),\n",
    "            nn.BatchNorm1d(n_features)\n",
    "        )\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.block(x)\n",
    "        out += identity  \n",
    "        return self.activation(out)\n",
    "\n",
    "class DeepRiskNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepRiskNet, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResBlock(512, dropout_rate=0.4),\n",
    "            ResBlock(512, dropout_rate=0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            ResBlock(256, dropout_rate=0.3),\n",
    "            ResBlock(256, dropout_rate=0.3)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.output_head(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1024  \n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "\n",
    "train_dataset = LoanDataset(X_train, y)\n",
    "val_dataset = LoanDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "model = DeepRiskNet(X_train.shape[1]).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, steps_per_epoch=len(train_loader), epochs=EPOCHS)\n",
    "\n",
    "\n",
    "print(\"üöÄ Starting Training...\")\n",
    "best_val_auc = 0\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    all_val_preds = []\n",
    "    all_val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_val_b, y_val_b in val_loader:\n",
    "            X_val_b = X_val_b.to(device)\n",
    "            preds = model(X_val_b)\n",
    "            all_val_preds.extend(preds.cpu().numpy())\n",
    "            all_val_labels.extend(y_val_b.numpy())\n",
    "    \n",
    "    val_auc = roc_auc_score(all_val_labels, all_val_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss/len(train_loader):.4f} | Val ROC-AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    \n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"‚èπÔ∏è Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pth', weights_only=True))\n",
    "print(f\"üèÜ Best Val AUC: {best_val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = LoanDataset(X_train, y_train)\n",
    "train_dataset = LoanDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "model20 = DeepRiskNet(X_train.shape[1]).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, steps_per_epoch=len(train_loader), epochs=EPOCHS)\n",
    "\n",
    "\n",
    "print(\"üöÄ Starting Training...\")\n",
    "best_val_auc = 0\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model20.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    model20.eval()\n",
    "    all_val_preds = []\n",
    "    all_val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_val_b, y_val_b in val_loader:\n",
    "            X_val_b = X_val_b.to(device)\n",
    "            preds = model(X_val_b)\n",
    "            all_val_preds.extend(preds.cpu().numpy())\n",
    "            all_val_labels.extend(y_val_b.numpy())\n",
    "    \n",
    "    val_auc = roc_auc_score(all_val_labels, all_val_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss/len(train_loader):.4f} | Val ROC-AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    \n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"‚èπÔ∏è Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "\n",
    "# model.load_state_dict(torch.load('best_model_20.pth', weights_only=True))\n",
    "print(f\"üèÜ Best Val AUC: {best_val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚è≥ Training Logistic Regression...\")\n",
    "lr_model_2 = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "lr_model_2.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚è≥ Training Gaussian Naive Bayes...\")\n",
    "nb_model_20 = GaussianNB()\n",
    "nb_model_20.fit(X_val, y_val)\n",
    "\n",
    "# y_val_probs_nb_20 = nb_model_20.predict_proba(X_val)[:, 1]\n",
    "# nb_auc = roc_auc_score(y_val, y_val_probs_nb)\n",
    "print(f\"‚úÖ Naive Bayes 20 Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Separate Submission Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_submission(ids, predictions, filename, method_name):\n",
    "    sub_df = pd.DataFrame({'ProfileID': ids, 'RiskFlag': predictions})\n",
    "    sub_df.to_csv(filename, index=False)\n",
    "    print(f\"‚úÖ Generated '{filename}' ({method_name})\")\n",
    "\n",
    "\n",
    "test_tensor = torch.tensor(X_test_processed, dtype=torch.float32).to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    nn_probs = model(test_tensor).cpu().numpy().flatten()\n",
    "    nn_preds = (nn_probs > 0.5).astype(int)\n",
    "create_submission(test_ids, nn_preds, 'finance_sub/submission_nn.csv', 'Neural Network')\n",
    "\n",
    "\n",
    "svm_preds = svm_pipeline.predict(X_test_processed)\n",
    "create_submission(test_ids, svm_preds, 'finance_sub/submission_svm.csv', 'SVM')\n",
    "\n",
    "\n",
    "lr_preds = lr_model.predict(X_test_processed)\n",
    "create_submission(test_ids, lr_preds, 'finance_sub/submission_lr.csv', 'Logistic Regression')\n",
    "\n",
    "\n",
    "nb_preds = nb_model.predict(X_test_processed)\n",
    "create_submission(test_ids, nb_preds, 'finance_sub/submission_nb.csv', 'Naive Bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚è≥ Training SVM... \")\n",
    "\n",
    "\n",
    "feature_map = Nystroem(kernel='rbf', gamma=0.1, n_components=2500, random_state=42)\n",
    "\n",
    "\n",
    "svm_clf_20 = LinearSVC(dual=False, C=1.0, class_weight='balanced', max_iter=2000, random_state=42)\n",
    "\n",
    "\n",
    "svm_pipeline_1 = Pipeline([\n",
    "    ('feature_map', feature_map),\n",
    "    ('svm_calibrated', CalibratedClassifierCV(svm_clf_20, cv=3)) \n",
    "])\n",
    "\n",
    "svm_pipeline_1.fit(X_val, y_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "results = []  \n",
    "\n",
    "\n",
    "val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    nn_probs = model(val_tensor).cpu().numpy().flatten()\n",
    "    nn_preds = (nn_probs > 0.5).astype(int)\n",
    "\n",
    "nn_f1 = f1_score(y_val, nn_preds, average=\"weighted\")\n",
    "results.append([\"Neural Network\", f\"{nn_f1:.4f}\"])\n",
    "\n",
    "\n",
    "val_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "model20.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    nn_probs = model20(val_tensor).cpu().numpy().flatten()\n",
    "    nn_preds = (nn_probs > 0.5).astype(int)\n",
    "\n",
    "nn_f1_20 = f1_score(y_train, nn_preds, average=\"weighted\")\n",
    "results.append([\"Neural Network 20\", f\"{nn_f1_20:.4f}\"])\n",
    "\n",
    "svm_preds = svm_pipeline.predict(X_val)\n",
    "svm_f1 = f1_score(y_val, svm_preds, average=\"weighted\")\n",
    "results.append([\"SVM\", f\"{svm_f1:.4f}\"])\n",
    "\n",
    "svm_preds2 = svm_pipeline_1.predict(X_train)\n",
    "svm_f2 = f1_score(y_train, svm_preds2, average=\"weighted\")\n",
    "results.append([\"SVM20\", f\"{svm_f2:.4f}\"])\n",
    "\n",
    "lr_preds = lr_model.predict(X_val)\n",
    "lr_f1 = f1_score(y_val, lr_preds, average=\"weighted\")\n",
    "results.append([\"Logistic Regression\", f\"{lr_f1:.4f}\"])\n",
    "\n",
    "lr_preds = lr_model_2.predict(X_train)\n",
    "lr_f1 = f1_score(y_train, lr_preds, average=\"weighted\")\n",
    "results.append([\"Logistic Regression 20\", f\"{lr_f1:.4f}\"])\n",
    "\n",
    "nb_preds = nb_model.predict(X_val)\n",
    "nb_f1 = f1_score(y_val, nb_preds, average=\"weighted\")\n",
    "results.append([\"Naive Bayes\", f\"{nb_f1:.4f}\"])\n",
    "\n",
    "nb_preds = nb_model_20.predict(X_val)\n",
    "nb_f1_20 = f1_score(y_val, nb_preds, average=\"weighted\")\n",
    "results.append([\"Naive Bayes 20\", f\"{nb_f1_20:.4f}\"])\n",
    "\n",
    "\n",
    "print(tabulate(results, headers=[\"Model\", \"F1 Score\"], tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20% SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mini_train, X_mini_test, y_mini_train, y_mini_test=train_test_split(X_mini_processed, y_mini, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚è≥ Training SVM... \")\n",
    "\n",
    "\n",
    "feature_map = Nystroem(kernel='rbf', gamma=0.1, n_components=2500, random_state=42)\n",
    "\n",
    "\n",
    "svm_clf20 = LinearSVC(dual=False, C=1.0, class_weight='balanced', max_iter=2000, random_state=42)\n",
    "\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    ('feature_map', feature_map),\n",
    "    ('svm_calibrated', CalibratedClassifierCV(svm_clf20, cv=3)) \n",
    "])\n",
    "\n",
    "svm_pipeline.fit(X_mini_train, y_mini_train)\n",
    "\n",
    "\n",
    "print(f\"‚úÖ SVM Training Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20% Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LoanDataset(Dataset):\n",
    "    def __init__(self, features, labels=None):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.float32).unsqueeze(1) if labels is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.features[idx], self.labels[idx]\n",
    "        return self.features[idx]\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_features, dropout_rate=0.3):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features),\n",
    "            nn.BatchNorm1d(n_features),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(n_features, n_features),\n",
    "            nn.BatchNorm1d(n_features)\n",
    "        )\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.block(x)\n",
    "        out += identity  \n",
    "        return self.activation(out)\n",
    "\n",
    "class DeepRiskNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepRiskNet, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResBlock(512, dropout_rate=0.4),\n",
    "            ResBlock(512, dropout_rate=0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            ResBlock(256, dropout_rate=0.3),\n",
    "            ResBlock(256, dropout_rate=0.3)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.output_head(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1024  \n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_train_dataset = LoanDataset(X_mini_train, y_mini_train)\n",
    "\n",
    "mini_train_loader = DataLoader(\n",
    "    mini_train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "model20 = DeepRiskNet(X_mini_train.shape[1]).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model20.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=LEARNING_RATE,\n",
    "    steps_per_epoch=len(mini_train_loader),\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting Training on X_mini ONLY...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model20.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in mini_train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model20(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss/len(mini_train_loader):.4f}\")\n",
    "\n",
    "print(\"üèÅ Training complete on mini dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_test_tensor = torch.tensor(X_mini_test, dtype=torch.float32).to(device)\n",
    "model20.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    nn_probs = model20(X_test_tensor).cpu().numpy().flatten()\n",
    "    nn_preds = (nn_probs > 0.5).astype(int)\n",
    "\n",
    "nn_f1 = f1_score(y_mini_test, nn_preds, average=\"weighted\")\n",
    "results.append([\"Neural Network (mini)\", f\"{nn_f1:.4f}\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "svm_preds = svm_pipeline.predict(X_mini_test)\n",
    "svm_f1 = f1_score(y_mini_test, svm_preds, average=\"weighted\")\n",
    "results.append([\"SVM (mini)\", f\"{svm_f1:.4f}\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(tabulate(results, headers=[\"Model\", \"Test F1 Score\"], tablefmt=\"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
